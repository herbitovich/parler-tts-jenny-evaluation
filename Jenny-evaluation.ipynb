{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a514f2e-1111-48c3-a0e1-8edf5eb01338",
   "metadata": {},
   "source": [
    "# Оценка качества синтеза модели Parler-TTS Mini v0.1 - Jenny\n",
    "\n",
    "## Шаг 1. Метрики\n",
    "Оценивать синтез речи, произведённый моделью, будем по следующим характеристикам: схожесть со спикером (speaker-similarity), качество речи (speech quality), надежность (robustness, то есть оценка того, насколько транскрипция сгенерированной аудизаписи содержательно схожа с референсом), скорость инференса. Для оценки модели по описанным выше характеристикам были выбраны следующие метрики:\n",
    "\n",
    "* speaker-similarity: SIM-O. Эта метрика была выбрана, как более объективная и воспроизводимая по сравнению с аналоговой SIM-R, более подробно можно почитать в разделе SIM [этой исследовательской работы](https://arxiv.org/pdf/2407.08551).\n",
    "* speech quality: MOS, запредикченная системой [UTMOS](https://github.com/sarulab-speech/UTMOS22), выбранной по [результатам](https://arxiv.org/pdf/2203.11389) сравнения с аналогами по объективным показателям\n",
    "* robustness: WER (Word Error Rate), выбранная как метрика объективная, легкая для замера и интерпретации.\n",
    "\n",
    "## Шаг 2. Работа с данными\n",
    "Для выбора оптимальных для оценки синтеза модели выделенными метриками аудиозаписей из датасета были обозначены следующие принципы:\n",
    "\n",
    "* Синтаксическое разнообразие: тексты разной структуры и длины\n",
    "* Лексическое разнообразие: различные стили речи и темы\n",
    "* Фонетическое разнообразие: содержание широго спектра звуков и фонем\n",
    "\n",
    "Загрузим датасет, возьмём рандомизированный семпл тысячи записей для оптимизации дальнейшей выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3263832b-663c-41b2-b9bd-bb35e74e9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "ds = load_dataset(\"reach-vb/jenny_tts_dataset\")\n",
    "batch_size = 1000\n",
    "sampled_data = ds['train'].shuffle(seed=42).select(range(1000))\n",
    "del ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c508e109-37f0-4136-ba96-57d082c615ff",
   "metadata": {},
   "source": [
    "Как работает оценка аудиозаписей по выбранным параметрам?<br>\n",
    "`cmudict` из модуля `nltk` для процессинга натурального языка позволяет извлечь фонемы из слова, обернём это в вспомогательную функцию `get_unique_phonetics`.<br>\n",
    "Основная функция `select_diverse_sentences` предназначена для извлечения `num_to_select` записей из датасета таких, что разброс следующих трех параметров среди них максимален: процент пересечения слов с уже выбранными, процент пересечения фонем с уже выбранными, длина предложения. Для этого был написан жадный алгоритм, отбирающий на каждом шаге лучший вариант аудиозаписи (т.е. самый отдаленный) из всех оставшихся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8588639-481e-4fd2-ae73-841f6edeca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict\n",
    "phonetics = cmudict.dict()\n",
    "\n",
    "\n",
    "def preprocess_sentence(sentence): # Пре-процессинг предложений: убираем пунктуацию\n",
    "    return sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "def get_unique_phonetics(words):\n",
    "    unique_phonetics = set()\n",
    "    for word in words:\n",
    "        phonetic = phonetics.get(word.lower()) # Сохраняем все фонемы каждого из слов в сет фонем всего предложения\n",
    "        if phonetic: # Некоторых слов может не быть в словаре\n",
    "            for ph in phonetic[0]:\n",
    "                unique_phonetics.add(ph)\n",
    "    return unique_phonetics # Возвращаем полученные уникальные фонемы\n",
    "\n",
    "\n",
    "    \n",
    "def select_diverse_sentences(sequence, num_to_select): # Выбираем num_to_select максимально удаленных друг от друга записей по параметрам длины, лексики и звуков\n",
    "\n",
    "    selected_audios = [] # Список выбранных записей\n",
    "\n",
    "    sounds = []\n",
    "    words = []\n",
    "    \n",
    "    for entry in sequence: # Предварительно итерируемся по всему датасету, чтобы для каждой записи вычислить сеты её фонем и слов\n",
    "        sentence = set(preprocess_sentence(entry['transcription_normalised']).split())\n",
    "        sounds.append(get_unique_phonetics(sentence))\n",
    "        words.append(sentence)\n",
    "\n",
    "    filenames = set() # Для проверки того, взяли ли мы какую-то запись (имена файлов в датасете уникальны)\n",
    "    taken_sounds = set()\n",
    "    taken_words = set()\n",
    "    taken_length_sum = 0\n",
    "    taken_amount = 0\n",
    "\n",
    "    def update(index): # Функция добавления свеже-выбранной записи\n",
    "        nonlocal selected_audios, filenames, taken_sounds, taken_words, taken_length_sum, taken_amount\n",
    "        selected_audios.append(sequence[index])\n",
    "        filenames.add(sequence[index]['file_name'])\n",
    "        taken_sounds |= sounds[index]\n",
    "        taken_words |= words[index]\n",
    "        taken_length_sum += len(words[index])\n",
    "        taken_amount += 1\n",
    "\n",
    "    update(random.randint(0, len(sequence)-1)) # Первая запись выбрана случайно\n",
    "    for time in range(num_to_select-1):\n",
    "        print(f\"{len(selected_audios)}/{num_to_select} аудиозаписей выбрано.\")\n",
    "        max_distance = float('-inf') # Критерий выбора (сумма значений всех параметров)\n",
    "    \n",
    "        for index in range(len(sequence)):\n",
    "            if sequence[index]['file_name'] not in filenames:\n",
    "                # Отношение пересечения сетов слов предложения и всех взятых к сету слов этого предложения\n",
    "                # (пытаемся выбрать такое предложение, которое привносит максимальную долю еще не встреченных нами слов)\n",
    "                lexical_coeff = len(words[index] - taken_words)/len(words[index])*1000 \n",
    "\n",
    "                \n",
    "                if sounds[index]:\n",
    "                    # Похожая схема, но с долей фонем\n",
    "                    phonetical_coeff = len(sounds[index] - taken_sounds)/len(sounds[index])*1000\n",
    "                else:\n",
    "                    phonetical_coeff = 0\n",
    "\n",
    "                #Пытаемся выбрать такое предложение, длина которого внесет наибольших вклад в среднее арифметическое длин\n",
    "                curr_mean = taken_length_sum/taken_amount\n",
    "                try:\n",
    "                    length_coeff = (curr_mean/abs(curr_mean - (taken_length_sum+len(words[index]))/(taken_amount+1)))*1000\n",
    "                except ZeroDivisionError:\n",
    "                    length_coeff = 0\n",
    "\n",
    "                    \n",
    "                distance = lexical_coeff + phonetical_coeff + length_coeff\n",
    "\n",
    "                # Нашли вариант лучше выбранного в данный момент\n",
    "                if distance > max_distance:\n",
    "                    max_distance = distance\n",
    "                    best_index = index\n",
    "\n",
    "        update(best_index)\n",
    "\n",
    "    return selected_audios\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe2471b-c024-4a82-93ad-b18ed2eabf44",
   "metadata": {},
   "source": [
    "Выберем 100 оптимальных для нас записей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e2079f-6039-4cfb-afab-06d987549359",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = select_diverse_sentences(sampled_data, 100)\n",
    "del sampled_data # Удаляем неиспользуемые переменные, чтобы освободить память\n",
    "del phonetics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757bb15-e0d7-4e1d-8ab6-48a35aef2ca2",
   "metadata": {},
   "source": [
    "Получили датасет, на котором будем оценивать модель.\n",
    "## Шаг 3. Инференс\n",
    "Сгенерируем аудиозаписи из полученных нами на предыдущем шаге промптов из датасета.\n",
    "На этом шаге нам было бы важно поэксперементировать с текстовым описанием, передаваемым в модель, если бы датасет, на котором тренировали модель был бы разнообразным: разные спикеры, эмоции, бэкграунд-звуки; но это не наш случай. Было подобрано нейтральное описание, соответствующее общему тону аудиозаписей датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4e768-e083-49a7-a881-058fc0cf39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from parler_tts import ParlerTTSForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "import soundfile as sf\n",
    "import time\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = ParlerTTSForConditionalGeneration.from_pretrained(\"parler-tts/parler-tts-mini-jenny-30H\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"parler-tts/parler-tts-mini-jenny-30H\")\n",
    "\n",
    "inferences = []\n",
    "description = \"Jenny speaks in a very confined sounding environment with clear audio quality.\"\n",
    "input_ids = tokenizer(description, return_tensors=\"pt\").input_ids.to(device)\n",
    "for index, entry in enumerate(dataset):\n",
    "    start_time = time.time() # Для замера времени инференса\n",
    "    \n",
    "    prompt = entry['transcription_normalised']\n",
    "    prompt_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids) # Генерируем синтезированную речь \n",
    "    \n",
    "    audio_arr = generation.cpu().numpy().squeeze()\n",
    "    entry['audio']['array_synthesized'] = audio_arr # Сохраняем векторное представление записи\n",
    "    inference_time = time.time() - start_time\n",
    "    audio_duration = len(audio_arr) / model.config.sampling_rate  # Вычисляем длительность записи в секундах\n",
    "    inferences.append(inference_time / audio_duration)\n",
    "    sf.write(f\"{index}_synthesized.wav\", audio_arr, model.config.sampling_rate) # Сохраняем аудиозапись в .wav\n",
    "    print(f\"{index+1}/{len(dataset)} записей обработано.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aee4fb-709e-437e-bdf4-83da6541bed9",
   "metadata": {},
   "source": [
    "## Шаг 4. Проводим оценку модели\n",
    "Самая интересная часть проекта, в которой нам необходимо произвести техническую реализацию оценки модели по выбранным нам характеристикам: натуральность звучания речи, схожесть звучания со спикером, содержательное совпадение промпта с транскрипцией речи модели.<br>\n",
    "### MOS - Mean Opinion Score\n",
    "Эта метрика представляет из себя человеческую субъективную оценку натуральности синтезированной речи, что значит, что автоматизировать её алгоритмически - очень сложная, если не невозможная задача. Очевидно, для этого нам нужно использовать пре-тренированные модели. Лучшим вариантом, как уже было сказано выше, оказалась система UTMOS. Примечательно, что несмотря на то, что её код целиком написан на Python, у системы UTMOS нет Python API из коробки - только CLI. Поэтому рабочим вариантом стал репозиторий [SpeechMOS](https://github.com/tarepan/SpeechMOS).\n",
    "### WER - Word Error Rate\n",
    "Крайне простая метрика в замере метрика - при подаче нам двух текстов: оригинала и синтезированного, нам нужно учесть, какая доля слов была \"проглочена\" в процессе синтеза, какая - придумана, и, наконец, какая изменена.<br>\n",
    "Проблема заключается лишь в том, чтобы эти текста получить: оригинал нам уже дан, а вот синтезированный нам придется, очевидно, доставать из сгенерированной нами на предыдущем шаге аудиозаписи, т.е. произвести транскрибацию. В этом нам поможет модель `hubert-large-ls960-ft`, с помощью которой мы сначала пре-процессим векторное представление записи для соответствия формату входных данных модели `CTC`, а потом скармливаем полученный массив, собственно, `hubert`. \n",
    "### SIM - SIMilarity with the Original\n",
    "Для этой метрики нам понадобится прогонять наши представления аудиозаписей через некоторую модель, чтобы получить эмбеддинг этой самой записи. С этим хорошо справляется `wav2vec-large-960h`, загруженная через дефолтную base-модель модуля transformers. Затем - просто высчитать расстояние между двумя векторами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa72d6-54eb-4613-8844-06bfe9fab795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model, HubertForCTC\n",
    "from scipy.spatial.distance import cosine # Для замера расстояния между векторами\n",
    "\n",
    "mos_predictor = torch.hub.load(\"tarepan/SpeechMOS:v1.2.0\", \"utmos22_strong\", trust_repo=True) # MOS predictor from UTMOS\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\") # For transcription (WER)\n",
    "embedding_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h\") # For embedding (SIM-O)\n",
    "transcription_model = HubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\") # For transcription (WER)\n",
    "\n",
    "def get_embeddings(waveform, sample_rate):\n",
    "    waveform = torch.from_numpy(waveform).float()\n",
    "    \n",
    "    if sample_rate != 16000:\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "    \n",
    "    inputs = waveform.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(inputs).last_hidden_state.mean(dim=1).squeeze() # Прогоняем через модель\n",
    "    \n",
    "    # Возвращаем эмбеддинг\n",
    "    return outputs\n",
    "\n",
    "def get_transcription(waveform, sample_rate):\n",
    "    waveform = torch.from_numpy(waveform).float()\n",
    "\n",
    "    if sample_rate != 16000: # hubert принимает лишь записи с sample_rate 16000, так что наши придется переконвертировать\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "    \n",
    "    input_values = processor(waveform, return_tensors=\"pt\", sampling_rate=16000).input_values # Пре-процессим\n",
    "    logits = transcription_model(input_values).logits # Передаем в модель\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.decode(predicted_ids[0]) # Декодируем в человекочитаемый язык\n",
    "    return transcription\n",
    "\n",
    "def preprocess(text): # Для нормализации транскрибаций\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def wer(reference, hypothesis): # Алгоритм высчитывания Word Error Rate\n",
    "    reference = preprocess(reference)\n",
    "    hypothesis = preprocess(hypothesis)\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "\n",
    "    d = [[0] * (len(hyp_words) + 1) for _ in range(len(ref_words) + 1)]\n",
    "\n",
    "    for i in range(len(ref_words) + 1):\n",
    "        d[i][0] = i  # Deletion cost\n",
    "    for j in range(len(hyp_words) + 1):\n",
    "        d[0][j] = j  # Insertion cost\n",
    "\n",
    "    for i in range(1, len(ref_words) + 1):\n",
    "        for j in range(1, len(hyp_words) + 1):\n",
    "            cost = 0 if ref_words[i - 1] == hyp_words[j - 1] else 1\n",
    "            d[i][j] = min(d[i - 1][j] + 1,      # Deletion\n",
    "                           d[i][j - 1] + 1,      # Insertion\n",
    "                           d[i - 1][j - 1] + cost)  # Substitution\n",
    "\n",
    "    S = d[len(ref_words)][len(hyp_words)]\n",
    "    N = len(ref_words)\n",
    "    wer_value = S / N if N > 0 else 1\n",
    "\n",
    "    return wer_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c60feb-a721-4edc-88a4-6bba3ff2cb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {} # Словарь для удобного хранения метрик каждой записи\n",
    "\n",
    "for index, entry in enumerate(dataset):\n",
    "\n",
    "    # Высчитываем SIM\n",
    "    original_embedding = get_embeddings(entry['audio']['array'], model.config.sampling_rate)\n",
    "    generated_embedding = get_embeddings(entry['audio']['array_synthesized'], model.config.sampling_rate)\n",
    "    similarity = 1 - cosine(original_embedding.numpy(), generated_embedding.numpy())\n",
    "\n",
    "    # Высчитываем WER\n",
    "    original_transcription = entry['transcription_normalised']\n",
    "    print(\"Original transcription:\",original_transcription)\n",
    "    generated_transcription = get_transcription(entry['audio']['array_synthesized'], model.config.sampling_rate)\n",
    "    print(\"Generated transcription:\",generated_transcription)\n",
    "    wer_value = wer(original_transcription, generated_transcription)\n",
    "\n",
    "    # \"Высчитываем\" MOS :)\n",
    "    mos = mos_predictor(torch.from_numpy(entry['audio']['array_synthesized']).unsqueeze(0), model.config.sampling_rate).item()\n",
    "\n",
    "    metrics[index] = {\"sim\": similarity, \"wer\": wer_value, \"mos\": mos, \"inf/s\": inferences[index]}\n",
    "    print(f\"{index+1}/{len(dataset)} записей обработано. Метрики новой записи: {metrics[index]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39259936-93eb-4685-bce5-9fb347f76125",
   "metadata": {},
   "source": [
    "## Шаг 5. Визуализация данных и выводы\n",
    "Визуализируем полученные данные с помощью `matplotlib` и проанализируем их. Найдем средние значения всех метрик для всех записей одновременно; затем построим графики зависимости значений метрик от определенных параметров.<br>\n",
    "![title](img/metrics.png)\n",
    "<br> \n",
    "<strong>SIM</strong><br>\n",
    "Как можно заметить на графике средних значений метрик для всех записей, синтезированный голос почти не отличим по звучанию от голоса спикера, тенденцию чего следовало ожидать из-за природы тренировочного датасета, но, всё-таки, значение метрики SIM является удивительно высоким.<br>\n",
    "<strong>WER</strong><br>\n",
    "20% содержательных ошибок - неплохой результат для такого маленького датасета, но, помимо всего прочего, этот параметр имеет явную зависимость, которую мы рассмотрим позднее.<br>\n",
    "<strong>MOS</strong><br>\n",
    "По предсказанной системой UTMOS шкале MOS, синтез речи моделью хорошо себя показывает, выдавая среднее значение ~4 по оригинальной шкале 1-5, что отображает в общем правильную статистику - иногда в полученных аудиозаписях слышны некритичные ошибки генерации, но в целом - синтезированный голос сложно отличить от настоящего.\n",
    "### Поиск зависимостей\n",
    "Построим два графика, на которых попытаемся проследить зависимости значений метрик от характеристик: количества уникальных фонем в предложении и количества слов в предложении. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d34cdc5-afeb-40ce-994d-a03917671279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sounds = {}\n",
    "words = {}\n",
    "phonetics = cmudict.dict()\n",
    "for index,entry in enumerate(dataset):\n",
    "    wd = len(set(preprocess_sentence(entry['transcription_normalised']).split()))\n",
    "    ph = len(get_unique_phonetics(entry['transcription_normalised']))\n",
    "    if ph in sounds:\n",
    "        sounds[ph].append(index)\n",
    "    else:\n",
    "        sounds[ph] = [index]\n",
    "    if wd in words:\n",
    "        words[wd].append(index)\n",
    "    else:\n",
    "        words[wd] = [index]\n",
    "\n",
    "\n",
    "\n",
    "bar_width = 0.2\n",
    "# Позиции столбцов на оси X\n",
    "\n",
    "\n",
    "def plot_statistics(parameter: dict, legend):\n",
    "    data = {}\n",
    "    for key in sorted(parameter):\n",
    "        sim = []\n",
    "        wer = []\n",
    "        mos = []\n",
    "        inf = []\n",
    "        for index in parameter[key]:\n",
    "            sim.append(int(metrics[index][\"sim\"]*100)) # Нормализуем долю в проценты\n",
    "            wer.append(int(metrics[index][\"wer\"]*100))\n",
    "            mos.append(int(metrics[index][\"mos\"]*20)) # Нормализуем шкалу 1-5 \n",
    "        sim = int(sum(sim)/len(sim))\n",
    "        wer = int(sum(wer)/len(wer))\n",
    "        mos = int(sum(mos)/len(mos))\n",
    "        data[key] = [sim, wer, mos, inf]\n",
    "    r1 = range(len(data))\n",
    "    r2 = [x + bar_width for x in r1]\n",
    "    r3 = [x + bar_width for x in r2]\n",
    "    r4 = [x + bar_width for x in r3]\n",
    "    fig, ax = plt.subplots()\n",
    "    bars1 = ax.bar(r1, [data[key][0] for key in data], color='b', width=bar_width, label='SIM (%)')\n",
    "    bars2 = ax.bar(r2, [data[key][1] for key in data], color='r', width=bar_width, label='WER (%)')\n",
    "    bars3 = ax.bar(r3, [data[key][2] for key in data], color='g', width=bar_width, label='MOS (1-5)')\n",
    "\n",
    "    ax.set_xlabel(legend)\n",
    "    ax.set_ylabel('Значение метрик')\n",
    "    ax.set_title('Зависимость метрик от критерия: '+ legend)\n",
    "    ax.set_xticks([r + bar_width for r in range(len(data))])\n",
    "    ax.set_xticklabels(data.keys())\n",
    "\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_statistics(sounds, \"Количество уникальных фонем\")\n",
    "plot_statistics(words, \"Длина предложения (в словах)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb5548-9a8d-4cb3-9fd3-90d01d7fd295",
   "metadata": {},
   "source": [
    "Рассмотрим получившиеся графики:<br>\n",
    "![title](img/words.png)<br>\n",
    "![title](img/phonetical.png)<br>\n",
    "Ожидаемо, что чарты метрики SIM незначительно колеблются на обоих графиках, остаются в одном и том же маленьком диапазоне (90%-100%).<br>\n",
    "На двух оставшихся метриках видим их явную корреляцию и зависимость от сложности предложений - как фонетической, так и лексической. Модель около безошибочно синтезирует озвучку однообразных маленьких текстов, но выдаёт все более плохие результаты по мере увеличения их сложности.\n",
    "<br><br>\n",
    "<i>Примечание: интересно, что по какой-то причине, значение метрики WER строится исключительно на \"проглатывании\" слов и фраз моделью - иногда она попросту игнорирует бОльшую часть промпта, но обрывание при этом происходит, по большей части, на моментах интонационной паузы.</i><br><br>\n",
    "Подводя итоги, можно сказать, что модель Parler-TTS Mini Jenny показывает себя удивительно хорошо по общепринятым характеристикам, и в особенности при инференсе на несложных по размеру и содержании текстах. При скалировании промпта надежность аутпута модели сильно падает."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts_env",
   "language": "python",
   "name": "tts_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
